# Mars Classification with CLIP, SigLIP & Vision Language Models (VLM)

Fine-tune CLIP, SigLIP image encoders, and train Vision Language Models (VLMs) on Mars datasets for planetary science research. Supports both multi-class terrain classification and binary classification tasks.

## ðŸš€ **NEW: Vision Language Model (VLM) Training**

The framework now supports training **Vision Language Models** for image classification with custom prompts and reasoning capabilities.

### VLM Quick Start

```bash
# Train VLM on Mars dataset with custom prompts
./run_vlm_training.sh \
    --dataset "Mirali33/mb-domars16k" \
    --class-names "ael aec cli rid fsf sfe fsg fse fss cra sfx mix rou smo tex" \
    --system-instructions "You are an expert Martian geologist AI..." \
    --prompt-template "Classify the Martian surface landform..." \
    --max-samples 100 \
    --epochs 3

# Train with default settings
./run_vlm_training.sh --dataset "Mirali33/mb-domars16k" --max-samples 50
```

### Available VLM Models

| Model | Parameters | Description |
|-------|------------|-------------|
| `HuggingFaceTB/SmolVLM-256M-Instruct` | 256M | Fastest, basic performance |
| `HuggingFaceTB/SmolVLM-500M-Instruct` | 500M | **Recommended** - good balance |
| `HuggingFaceTB/SmolVLM-Instruct` | 2.2B | Best performance, slower |
| `microsoft/Phi-3.5-vision-instruct` | 4.2B | Strong performance |
| `Qwen/Qwen2-VL-2B-Instruct` | 2B | Excellent reasoning |

### VLM Features

- âœ… **Custom Prompts**: Define system instructions and prompt templates
- âœ… **Reasoning**: Models provide step-by-step reasoning before classification
- âœ… **Any Dataset**: Works with any HuggingFace image classification dataset
- âœ… **Auto-detection**: Automatically detects image/label columns and class names
- âœ… **Memory Optimized**: CPU training with MPS memory management
- âœ… **Batch Processing**: Custom collation for variable-length sequences

### VLM Training Example

```bash
# Mars terrain classification with expert prompts
./run_vlm_training.sh \
    --dataset "Mirali33/mb-domars16k" \
    --model "HuggingFaceTB/SmolVLM-500M-Instruct" \
    --class-names "ael aec cli rid fsf sfe fsg fse fss cra sfx mix rou smo tex" \
    --system-instructions "You are an expert Martian geologist AI. Your task is to classify Martian surface landform images. You will be provided with an image of a Martian surface landform. You must respond with ONLY the three-letter abbreviation of the most prominent landform class present in the image. Here are the possible landform classes: Aeolian Bedforms: - (ael) Aeolian Curved: Wind-formed bedforms with a curved, dune-like, or rippled appearance. - (aec) Aeolian Straight: Wind-formed bedforms with a straight, linear, or elongated ridge-like appearance. Topographic Landforms: - (cli) Cliff: A steep, near-vertical, or very abrupt rock exposure or slope. - (rid) Ridge: An elongated, narrow elevation or crest of land. - (fsf) Channel: A depression, groove, or trough, often suggesting past fluid flow. - (sfe) Mounds: Distinct, rounded, or irregularly shaped raised landforms. Slope Feature Landforms: - (fsg) Gullies: Small, incised channels or ravines, typically found on slopes. - (fse) Slope Streaks: Dark or light markings that appear on slopes. - (fss) Mass Wasting: Features resulting from the downslope movement of rock. Impact Landforms: - (cra) Crater: A bowl-shaped depression formed by an impact event. - (sfx) Crater Field: An area characterized by a significant concentration of impact craters. Basic Terrain Landforms: - (mix) Mixed Terrain: An area exhibiting a combination of characteristics. - (rou) Rough Terrain: An area characterized by irregular, uneven surfaces. - (smo) Smooth Terrain: An area characterized by relatively even surfaces. - (tex) Textured Terrain: An area exhibiting a distinct surface pattern. Analyze the provided image and output only the three-letter abbreviation for the dominant landform." \
    --prompt-template "Classify the Martian surface landform in the following image. Strictly use this format: Reasoning: [step-by-step reasoning] Answer: [Provide only the three-letter abbreviation for the dominant landform type]" \
    --max-samples 200 \
    --epochs 2 \
    --batch-size 1
```

## Supported Datasets

### Default: Mars Terrain Classification
The [mb-domars16k dataset](https://huggingface.co/datasets/Mirali33/mb-domars16k) contains 29,718 Mars terrain images across 15 terrain classes:

- **ael**: Aeolian features, **rou**: Rough terrain, **cli**: Cliffs
- **aec**: Aeolian crater, **tex**: Textured terrain, **smo**: Smooth terrain  
- **fss**: Frost/snow surfaces, **rid**: Ridges, **fse**: Frost/snow edges
- **sfe**: Sand/frost edges, **fsf**: Frost/snow formations, **fsg**: Frost/snow gullies
- **sfx**: Sand/frost mixed, **cra**: Craters, **mix**: Mixed terrain

### Custom Datasets
The framework supports any HuggingFace dataset with image classification format:
- **Binary classification**: Rock vs non-rock, crater vs no-crater, etc.
- **Multi-class**: Any number of terrain/feature classes
- **Custom splits**: Handles train/val/test or train-only datasets

## Available Models

### CLIP & SigLIP Models

| Model | Description | Hidden Size |
|-------|-------------|-------------|
| `clip-vit-base-patch32` | CLIP ViT-B/32 (default) | 768 |
| `clip-vit-base-patch16` | CLIP ViT-B/16 | 768 |
| `clip-vit-large-patch14` | CLIP ViT-L/14 | 1024 |
| `siglip-base-patch16-224` | SigLIP Base | 768 |
| `siglip-large-patch16-256` | SigLIP Large | 1024 |

### Vision Language Models (VLM)

| Model | Parameters | Max Length | Description |
|-------|------------|------------|-------------|
| `HuggingFaceTB/SmolVLM-256M-Instruct` | 256M | 2048 | Fastest inference |
| `HuggingFaceTB/SmolVLM-500M-Instruct` | 500M | 2048 | **Recommended** |
| `HuggingFaceTB/SmolVLM-Instruct` | 2.2B | 2048 | Best performance |
| `microsoft/Phi-3.5-vision-instruct` | 4.2B | 4096 | Strong reasoning |
| `Qwen/Qwen2-VL-2B-Instruct` | 2B | 32768 | Complex reasoning |

## Setup

```bash
# Create virtual environment
python -m venv mars_env
source mars_env/bin/activate  # On Windows: mars_env\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

## Quick Start

### 1. VLM Training (NEW)

```bash
# Make scripts executable
chmod +x run_vlm_training.sh
chmod +x run_training.sh

# Train VLM with default settings
./run_vlm_training.sh --dataset "Mirali33/mb-domars16k" --max-samples 100

# Train VLM with custom prompts
./run_vlm_training.sh \
    --dataset "Mirali33/mb-domars16k" \
    --class-names "ael aec cli rid fsf sfe fsg fse fss cra sfx mix rou smo tex" \
    --system-instructions "You are an expert Martian geologist..." \
    --prompt-template "Classify the Martian surface landform..." \
    --max-samples 200
```

### 2. CLIP/SigLIP Training

```bash
# Train with default settings
./run_training.sh

# Train with custom parameters
./run_training.sh --model clip-vit-base-patch32 --epochs 10 --batch-size 16
```

### 3. Train on Custom Dataset

```bash
# VLM training on custom dataset
./run_vlm_training.sh \
    --dataset your-username/custom-dataset \
    --class-names "class1 class2 class3" \
    --task-description "custom classification task"

# CLIP training on custom dataset
python train.py \
    --dataset your-username/mars-binary-dataset \
    --num_classes 2 \
    --class_names rock no-rock \
    --model clip-vit-base-patch32
```

### 4. Run Inference

```bash
# VLM inference
python inference_vlm.py \
    --model_path ./vlm_results/best_model \
    --image_path mars_image.jpg \
    --visualize

# CLIP inference
python inference.py \
    --model_path ./clip_results/best_model.pth \
    --model_key clip-vit-base-patch32 \
    --image_path mars_image.jpg \
    --visualize
```

## Training Options

### VLM Training Arguments

```bash
./run_vlm_training.sh \
    --model HuggingFaceTB/SmolVLM-500M-Instruct \
    --dataset Mirali33/mb-domars16k \
    --batch-size 1 \
    --epochs 3 \
    --learning-rate 1e-5 \
    --class-names "ael aec cli rid fsf sfe fsg fse fss cra sfx mix rou smo tex" \
    --system-instructions "Custom system instructions..." \
    --prompt-template "Custom prompt template..." \
    --max-samples 100 \
    --use-wandb
```

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--model` | VLM model to use | `HuggingFaceTB/SmolVLM-500M-Instruct` |
| `--dataset` | HuggingFace dataset name | `Mirali33/mb-domars16k` |
| `--class-names` | List of class names | Auto-detected |
| `--system-instructions` | Custom system instructions | Auto-generated |
| `--prompt-template` | Custom prompt template | Auto-generated |
| `--batch-size` | Training batch size | 1 |
| `--epochs` | Number of epochs | 3 |
| `--learning-rate` | Learning rate | 1e-5 |
| `--max-samples` | Limit samples for testing | None |
| `--use-wandb` | Enable W&B logging | False |

### CLIP/SigLIP Training Arguments

```bash
python train.py \
    --model clip-vit-base-patch32 \
    --batch_size 16 \
    --num_epochs 10 \
    --learning_rate 5e-5 \
    --freeze_encoder \
    --use_wandb
```

| Parameter | Description | Default |
|-----------|-------------|---------|
| `--model` | Model architecture | `clip-vit-base-patch32` |
| `--dataset` | HuggingFace dataset name | `Mirali33/mb-domars16k` |
| `--batch_size` | Training batch size | 32 |
| `--num_epochs` | Number of epochs | 10 |
| `--learning_rate` | Learning rate | 5e-5 |
| `--freeze_encoder` | Freeze vision encoder weights | False |
| `--use_wandb` | Enable W&B logging | False |

## Recent Fixes & Improvements

### âœ… **Fixed Issues**

1. **Class Names Parsing**: Fixed shell script to correctly pass class names as separate arguments
2. **Memory Management**: Added MPS memory management for Apple Silicon GPUs
3. **Custom Collation**: Implemented proper batch collation for variable-length sequences
4. **CPU Training**: Force CPU usage to avoid MPS memory issues
5. **Auto-detection**: Enhanced dataset auto-detection for image/label columns

### ðŸ”§ **Technical Improvements**

- **VLM Architecture**: Proper Vision Language Model implementation using `AutoModelForVision2Seq`
- **Prompt Engineering**: Support for custom system instructions and prompt templates
- **Reasoning Capabilities**: Models provide step-by-step reasoning before classification
- **Batch Processing**: Custom collation function for handling variable-length sequences
- **Memory Optimization**: Environment variables and device management for stable training

## Expected Performance

### VLM Performance
- **Training time**: 30 minutes - 2 hours (depending on model size)
- **Memory usage**: 1-4 GB (CPU training)
- **Expected accuracy**: 85-92% (with proper prompts)
- **Reasoning quality**: Step-by-step analysis before classification

### CLIP/SigLIP Performance
- **Training time**: 30 minutes - 2 hours
- **Expected accuracy**: 88-94%
- **Memory usage**: 2-8 GB (depending on model size)

## Model Architecture

### VLM Architecture
```
Input Image + Text Prompt
       â†“
Vision Language Model
  - Vision Encoder
  - Language Model
  - Cross-Attention
       â†“
Generated Text Response
  - Reasoning
  - Classification Answer
```

### CLIP/SigLIP Architecture
```
Input Image (224x224x3)
       â†“
Vision Encoder (CLIP/SigLIP)
       â†“
Visual Features (768/1024-dim)
       â†“
Classification Head
  - Dropout(0.1)
  - Linear(hidden â†’ hidden/2) + ReLU
  - Dropout(0.1)  
  - Linear(hidden/2 â†’ num_classes)
       â†“
Terrain Class Logits
```

## Results Structure

### VLM Results
```
vlm_results/
â”œâ”€â”€ best_model/              # Best model weights
â”œâ”€â”€ final_model/             # Final model weights
â”œâ”€â”€ training_metadata.json   # Training configuration
â”œâ”€â”€ results.json             # Evaluation metrics
â””â”€â”€ training_curves.png      # Training plots
```

### CLIP/SigLIP Results
```
clip_results/
â”œâ”€â”€ best_model.pth           # Best model weights
â”œâ”€â”€ final_model.pth          # Final model weights
â”œâ”€â”€ results.json             # Evaluation metrics
â””â”€â”€ training_curves.png      # Training plots
```

## Monitoring Training

### Using Weights & Biases

```bash
# Login (first time only)
wandb login

# Train with logging
./run_vlm_training.sh --use-wandb
./run_training.sh --use-wandb
```

### Console Output

Monitor training progress through:
- Training loss (should decrease)
- Validation accuracy (should increase)
- Learning rate schedule
- VLM reasoning quality (for VLM training)

## Tips for Better Results

### VLM Training Tips
- **Model Selection**: Start with SmolVLM-500M for good balance
- **Prompts**: Write clear, specific system instructions
- **Batch Size**: Use batch size 1 for memory efficiency
- **Reasoning**: Encourage step-by-step reasoning in prompts

### CLIP/SigLIP Training Tips
- **Model Selection**: CLIP ViT-B/32 for starting point
- **Learning Rate**: 5e-5 for full fine-tuning, 1e-3 for frozen encoder
- **Batch Size**: Use largest that fits in memory
- **Epochs**: 8-12 typically sufficient

## Troubleshooting

### Common Issues

**MPS Memory Issues (VLM)**
```bash
# Force CPU training (already implemented)
./run_vlm_training.sh --max-samples 50
```

**CUDA Out of Memory (CLIP/SigLIP)**
```bash
# Reduce batch size
./run_training.sh --batch-size 8
```

**Slow Training**
```bash
# Use frozen encoder (CLIP/SigLIP)
./run_training.sh --freeze-encoder

# Use smaller VLM model
./run_vlm_training.sh --model HuggingFaceTB/SmolVLM-256M-Instruct
```

### Debug Mode

```bash
# Quick test run (VLM)
./run_vlm_training.sh --max-samples 5 --epochs 1

# Quick test run (CLIP/SigLIP)
python train.py --num_epochs 1 --batch_size 4 --eval_steps 10
```

## File Structure

```
VLM/
â”œâ”€â”€ train_vlm.py              # VLM training script
â”œâ”€â”€ inference_vlm.py          # VLM inference script
â”œâ”€â”€ train.py                  # CLIP/SigLIP training script
â”œâ”€â”€ inference.py              # CLIP/SigLIP inference script
â”œâ”€â”€ run_vlm_training.sh       # VLM training helper script
â”œâ”€â”€ run_training.sh           # CLIP/SigLIP training helper script
â”œâ”€â”€ requirements.txt          # Dependencies
â””â”€â”€ README.md                # This file
```

## License

MIT License. The Mars dataset is licensed under CC-BY-4.0.

## Citation

```bibtex
@misc{mars-classification,
  title={Mars Terrain Classification with CLIP, SigLIP and Vision Language Models},
  year={2025},
  url={https://huggingface.co/datasets/Mirali33/mb-domars16k}
}
``` 